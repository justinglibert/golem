training:
  unroll_length: 80
  batch_size: 8
  total_steps: 1000000000
  entropy_cost: 0.0006
  baseline_cost: 0.5
  discounting: 0.99
  learning_rate: 0.00048
  alpha: 0.99
  momentum: 0.0
  epsilon: 0.01
  grad_norm_clipping: 40.0
  total_parameter_updates: 10
